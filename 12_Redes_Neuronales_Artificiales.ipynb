{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "\n",
    "## 1. Perceptrón\n",
    "\n",
    "Su origen viene en intentos de encontrar representaciones matemáticas de procesado de la información en sistemas biológicos.\n",
    "\n",
    "Aunque primera implementación data de 1947, realmente fue con la aparición del llamado **perceptrón**, gracias al trabajo de Frank Rosenblatt, en 1956 cuando empezó a coger fama. Incluso la prensa generalista se hizo eco de este hecho: https://www.newyorker.com/magazine/1958/12/06/rival-2:\n",
    "\n",
    "*Dr. Rosenblatt defined the perceptron as the first non-biological object which will achieve an organization o its external environment in a meaningful way. It interacts with its environment, forming concepts that have not been made ready for it by a human agent[...]It can tell the difference betw. a cat and a dog, although it wouldn't be able to tell whether the dog was to the left or right of the cat.*\n",
    "\n",
    "Un perceptrón no es más que una regresión lineal con una **función de activación** a la salida.\n",
    "\\\\[\n",
    "y(x)= f\\left( \\sum w_i · x_i\\right)\n",
    "\\\\]\n",
    "Donde la función de activación *f(x)* simplemente saca un 1 si el valor está por encima de un umbral o 0 si está por debajo.\n",
    "\n",
    "A veces se escribe como:\n",
    "\\\\[\n",
    "y(x)= f\\left( bias+\\sum w_i · x_i\\right)\n",
    "\\\\]\n",
    "Donde $bias$ es una constante que se añade a la función. Ambas fórmulas son equivalentes si una de las columnas de $x_i$ se deja como constante.\n",
    "\n",
    "Se inspira en como funciona una neurona:\n",
    "![](img/neurona.png)\n",
    "Se supone que una determinada combinación de señales en las dentridas puedel lanzar una señal en el axón.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/400px-Perceptr%C3%B3n_5_unidades.svg.png)\n",
    "El perceptrón es un clasificador binario, sirve para decir si un vector pertenece o no a una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height=2,repr.plot.width=8,repr.plot.res = 200)\n",
    "\n",
    "set.seed(12)\n",
    "Num_samples<-10\n",
    "\n",
    "dfa<-data.frame(group=\"A\",x1=rnorm(Num_samples,mean=0,sd=1),x2=rnorm(Num_samples,mean=0,sd=1))\n",
    "dfb<-data.frame(group=\"B\",x1=rnorm(Num_samples,mean=2.5,sd=1),x2=rnorm(Num_samples,mean=1.5,sd=1))\n",
    "df_train<-rbind(dfa,dfb)\n",
    "library(ggplot2)\n",
    "options(repr.plot.height=4,repr.plot.width=6)\n",
    "\n",
    "ggplot(df_train,aes(x=x1,y=x2,color=group))+geom_point(size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron<-function(x, weigths, threshold=0){\n",
    "    if ((weigths[1]+sum(x*weigths[2:length(weigths)]))>threshold){\n",
    "        return(1)\n",
    "    }else{\n",
    "        return(0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths<-c(1,1,1)\n",
    "threshold<-0.5\n",
    "pred<-apply(df_train[,2:3],1,function(x) perceptron(x,weigths=weigths,threshold=threshold))\n",
    "table(factor(pred),df_train$group)\n",
    "\n",
    "ggplot(df_train,aes(x=x1,y=x2,color=group))+geom_point(size=0.5)+\n",
    "    geom_abline(slope = -weigths[2]/weigths[3],intercept = (threshold-weigths[1])/weigths[3],color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold <- 0.5\n",
    "learning_rate <- 0.002\n",
    "\n",
    "w<-c(0.5,0,0.005)\n",
    "i<-0\n",
    "iter<-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i<-i %% (nrow(df_train)-1)+1\n",
    "iter<-iter+1\n",
    "    \n",
    "x_vector <- df_train[i,c(2:3)]\n",
    "y_out<-perceptron(x_vector,weigths = w,threshold=threshold)\n",
    "\n",
    "error=as.numeric(df_train[i,\"group\"]==\"B\")-y_out\n",
    "pre_w<-w\n",
    "if (error!=0){    \n",
    "    w[1]<-w[1]+error*learning_rate\n",
    "    for (idx in 1:length(x_vector)){\n",
    "        w[idx+1]<-w[idx+1]+x_vector[1,idx]*error*learning_rate        \n",
    "    }\n",
    "}\n",
    "paste(\"Iteracción\",iter,\"Dato:\",i,\"Error:\",error,\"Pesos:\",paste(w,collapse=\",\"))\n",
    "ggplot(df_train,aes(x=x1,y=x2,color=group))+geom_point(size=0.5)+\n",
    "    geom_point(size=3,x=x_vector[1,1],y=x_vector[1,2],shape=0,color=\"red\")+\n",
    "    geom_abline(slope = -pre_w[2]/pre_w[3],intercept = (threshold-pre_w[1])/pre_w[3],color='blue',linetype=3)+\n",
    "    geom_abline(slope = -w[2]/w[3],intercept = (threshold-w[1])/w[3],color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este algoritmo era muy sencillo computancionalmente y se puede conseguir grandes logros. Por ejemplo podemos reconocer caracteres numéricos escritos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"dslabs\")\n",
    "mnist_data<-read_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_row_image<-function(image_list,index){    \n",
    "    row<-image_list$images[index,]\n",
    "    label<-image_list$labels[index]\n",
    "    img<-matrix(row/255,ncol = sqrt(length(row)))\n",
    "    img<-t(apply(img,2,rev))\n",
    "    img<-t(apply(img,1,rev))\n",
    "    plot(1:2, type='n',main=label)\n",
    "    rasterImage(img, 1, 1, 2,2)\n",
    "}\n",
    "options(repr.plot.height=4,repr.plot.width=4)\n",
    "\n",
    "plot_row_image(mnist_data$train,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un clasificador que nos diga si un número es un 0 o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold <- 0.5\n",
    "learning_rate <- 0.0002\n",
    "\n",
    "num_pixel<-length(mnist_data$train$images[1,])\n",
    "\n",
    "#Incializamos los pesos a un valor aleatorio y cercano a 0\n",
    "w<-unlist(rnorm(num_pixel+1,mean=0,sd=0.1))\n",
    "i<-0\n",
    "iter<-0\n",
    "error_list<-c()\n",
    "epoch<-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epocs<-40\n",
    "pixels_value<-mnist_data$train$images\n",
    "labels_out<-as.numeric(mnist_data$train$label==0)\n",
    "\n",
    "repeat{\n",
    "    num_errors<-0    \n",
    "    epoch<-epoch+1\n",
    "    for (i in 1:nrow(pixels_value)){\n",
    "        iter<-iter+1\n",
    "\n",
    "        x_vector <- pixels_value[i,]\n",
    "        y_out<-perceptron(x_vector,weigths = w,threshold=threshold)\n",
    "\n",
    "        error=labels_out[i]-y_out        \n",
    "        if (error!=0){    \n",
    "            w[1]<-w[1]+error*learning_rate\n",
    "            for (idx in 1:length(x_vector)){\n",
    "                w[idx+1]<-w[idx+1]+x_vector[idx]*error*learning_rate        \n",
    "            }\n",
    "            num_errors<-num_errors+abs(error)\n",
    "        }\n",
    "    }\n",
    "    error_list<-c(error_list,num_errors)\n",
    "    if(epoch>max_epocs || num_errors==0){\n",
    "        break\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(error_list,xlab=\"epoch\",t=\"l\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images<-mnist_data$test$images\n",
    "test_label_real<-as.numeric(mnist_data$test$label==0)\n",
    "\n",
    "y_out<-apply(test_images,1,function(x) perceptron(x,weigths = w))\n",
    "library(caret)\n",
    "confusionMatrix(table(y_out,test_label_real),positive = \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.pinimg.com/236x/82/83/ec/8283ecb4ec4996e262f303de186a6786--robby-the-robot-mr-robot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo era felicidad, alegría, risas y optimismo hasta que en 1969 Marvin Minsky y Seymour A. Papert publicaron el siguiente libro:\n",
    "\n",
    "https://mitpress.mit.edu/books/perceptrons\n",
    "\n",
    "![](https://images-na.ssl-images-amazon.com/images/I/51Ok0iTK1ML._SX329_BO1,204,203,200_.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptron tiene un problema y es que solo puede clasificar grupos linealmente separables. Demostraron que el perceptrón no es capaz de converger en el simple problema XOR.\n",
    "\n",
    "<table>\n",
    "<tr><td style=\"padding:0 35px;\">\n",
    "    \n",
    "| A | B | XOR |\n",
    "|-|-|-| \n",
    "| 0 | 0 | 0 |      \n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "</td><td style=\"padding:0 35px;\">\n",
    "    \n",
    "| A | B | OR |\n",
    "|-|-|-| \n",
    "| 0 | 0 | 0 |      \n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "</td><td style=\"padding:0 35px;\">\n",
    "    \n",
    "| A | B | AND |\n",
    "|-|-|-| \n",
    "| 0 | 0 | 0 |      \n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "</td>\n",
    "</tr> </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(12)\n",
    "Num_samples<-5\n",
    "sdev<-0.05\n",
    "dfa1<-data.frame(group=\"A\",x1=rnorm(Num_samples,mean=0,sd=sdev),x2=rnorm(Num_samples,mean=0,sd=sdev))\n",
    "dfa2<-data.frame(group=\"A\",x1=rnorm(Num_samples,mean=1,sd=sdev),x2=rnorm(Num_samples,mean=1,sd=sdev))\n",
    "dfb1<-data.frame(group=\"B\",x1=rnorm(Num_samples,mean=0,sd=sdev),x2=rnorm(Num_samples,mean=1,sd=sdev))\n",
    "dfb2<-data.frame(group=\"B\",x1=rnorm(Num_samples,mean=1,sd=sdev),x2=rnorm(Num_samples,mean=0,sd=sdev))\n",
    "\n",
    "df_train<-rbind(dfa1,dfb1,dfa2,dfb2)\n",
    "#df_train<-df_train[sample(nrow(df_train)),]\n",
    "library(ggplot2)\n",
    "options(repr.plot.height=4,repr.plot.width=6)\n",
    "\n",
    "ggplot(df_train,aes(x=x1,y=x2,color=group))+geom_point(size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold <- 0.5\n",
    "learning_rate <- 0.002\n",
    "\n",
    "w<-c(0.5,0,0.005)\n",
    "i<-0\n",
    "iter<-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i<-i %% (nrow(df_train)-1)+1\n",
    "iter<-iter+1\n",
    "    \n",
    "x_vector <- df_train[i,c(2:3)]\n",
    "y_out<-perceptron(x_vector,weigths = w, threshold=threshold)\n",
    "\n",
    "error=as.numeric(df_train[i,\"group\"]==\"B\")-y_out\n",
    "pre_w<-w\n",
    "if (error!=0){    \n",
    "    w[1]<-w[1]+error*learning_rate\n",
    "    for (idx in 1:length(x_vector)){\n",
    "        w[idx+1]<-w[idx+1]+x_vector[1,idx]*error*learning_rate        \n",
    "    }\n",
    "}\n",
    "print(paste(\"Iteracción\",iter,\"Dato:\",i,\"Error:\",error,\"Pesos:\",paste(w,collapse=\",\")))\n",
    "ggplot(df_train,aes(x=x1,y=x2,color=group))+geom_point(size=0.5)+\n",
    "    geom_point(size=3,x=x_vector[1,1],y=x_vector[1,2],shape=0,color=\"red\")+\n",
    "    geom_abline(slope = -pre_w[2]/pre_w[3],intercept = (threshold-pre_w[1])/pre_w[3],color='blue',linetype=3)+\n",
    "    geom_abline(slope = -w[2]/w[3],intercept = (threshold-w[1])/w[3],color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# WINTER IS COMING\n",
    "\n",
    "La inteligencia artificial cae en popularidad, llega lo que se conoce como el invierto de la inteligencia artificial: https://en.wikipedia.org/wiki/AI_winter\n",
    "\n",
    "![](#img/decepcion.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptrón multicapa\n",
    "\n",
    "Para solucionar este tipo de problemas inventaron el perceptron multicapa que consiste en que la salida de un perceptrón sea la entrada de otro.\n",
    "\n",
    "![](img/xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=c(1,1)\n",
    "h1=perceptron(x,c(0,1,0))\n",
    "h2=perceptron(x,c(-1,1,1))\n",
    "h3=perceptron(x,c(0,0,1))\n",
    "c(h1,h2,h3)\n",
    "perceptron(c(h1,h2,h3),c(0,1,-2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora solo tenemos que crear una función nueva y entrenarla como habíamos hecho hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilayer_perceptron<-function(x,weigths){\n",
    "    h1=perceptron(x,w[c(1,2,3)])\n",
    "    h2=perceptron(x,w[c(4,5,6)])\n",
    "    h3=perceptron(x,w[c(7,8,9)])\n",
    "    perceptron(c(h1,h2,h3),w[c(10,11,12,13)])\n",
    "}\n",
    "x=c(1,0)\n",
    "w<-c(0,1,0,-1,1,1,0,0,1,0,1,-2,1)\n",
    "multilayer_perceptron(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate <- 0.02\n",
    "\n",
    "w<-rnorm(13,mean=0,sd=0.001)\n",
    "i<-0\n",
    "iter<-0\n",
    "errors<-c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold <- 0\n",
    "\n",
    "for (epoch in 1:40){\n",
    "    error_i<-0\n",
    "    for (i in 1:nrow(df_train)){    \n",
    "\n",
    "        x_vector <- df_train[i,c(2:3)]\n",
    "        y_out<-multilayer_perceptron(x_vector,weigths = w)\n",
    "\n",
    "        error=as.numeric(df_train[i,\"group\"]==\"B\")-y_out\n",
    "        if (error!=0){    \n",
    "            w[1]<-w[1]+error*learning_rate\n",
    "            for (idx in 1:length(x_vector)){\n",
    "                w[idx+1]<-w[idx+1]+x_vector[1,idx]*error*learning_rate        \n",
    "            }\n",
    "            #print(paste(\"Dato:\",i,\"Pesos:\",paste(w,collapse=\",\")))\n",
    "\n",
    "        }        \n",
    "        error_i<-error_i+error\n",
    "    }\n",
    "    \n",
    "    errors<-c(errors,error_i)\n",
    "}\n",
    "plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo no converge, no podemos entrenar un perceptron para calcular los pesos.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, ¿cómo se entrenan los pesos? El algoritmo que hemos visto ahora no sirve para perceptrones multicapa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation \n",
    "\n",
    "La solución llegó en 1986, el algoritmo backpropagation es capaz de calcular los pesos de una red neuronal de varias capas mirando como se propagan los errores.\n",
    "\n",
    "Se empiezan a usar dos nuevas funciones de activación:\n",
    "* sigmoide: $f(x)=\\frac{1}{1+e^{-x}}=\\frac{e^x}{1+e^x}$\n",
    "\n",
    "* tanh: $f(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x<-seq(-10,10,length.out = 100)\n",
    "\n",
    "plot(tanh(x),ylab='y',xlab='x',t='l',col='red')\n",
    "lines(1/(1+exp(-x)),ylab='y',xlab='x',t='l',col='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura de una red neuronal prealimentada (feed-fordward neural network) es la siguiente:\n",
    "\n",
    "![](img/NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estas redes se define una función error objetivo que hay que minimizar. Una común puede ser el error cuadrático medio que ya vimos en regresiones lineales, pero hay muchas otras en función del tipo de problema que se trate de atacar.\n",
    "\n",
    "El algoritmo de backpropagation se basa en la regla de la cadena a la hora de ir diferenciando como varía el error, *propaga* el error desde la salida hacia atrás hasta llegar a la entrada pasando por todas las capas ocultas. Este algoritmo se basa en la regla de la cadena:\n",
    "\n",
    "\\\\[\n",
    "\\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dz}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"dslabs\")\n",
    "mnist_data<-read_mnist()\n",
    "str(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train<-cbind(data.frame(digit0=mnist_data$train$labels==0),as.data.frame(mnist_data$train$images))\n",
    "head(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(neuralnet)\n",
    "set.seed(1)\n",
    "nn=neuralnet(digit0~., data=df_train, hidden=4,act.fct = \"logistic\",linear.output = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=compute(nn,as.data.frame(mnist_data$test$images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result<-data.frame(pred=pred$net.result,real=mnist_data$test$label==0)\n",
    "library(caret)\n",
    "caret::confusionMatrix(table(df_result$pred>0.4,df_result$real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales se convierten en la **segunda mejor** forma de afrontar este tipo de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deeplearning\n",
    "\n",
    "En 2006 científicos con mejor idea de como vender su trabajo empiezan a acuñar el termino deeplearning para referirse a redes neuronales con múltiples capas.\n",
    "\n",
    "Desde entonces han aparecido bastantes nuevos avances:\n",
    "* Mayor capacidad de cómputo: CPUs multicore, GPUs\n",
    "* Big Data: Muchos más datos para entrenar\n",
    "* Nuevos tipos de capas de neuronas:\n",
    " * Convolucionales\n",
    " * Memoria (LSTM, GRU)\n",
    "* Nuevas funciones de activación:\n",
    " * ReLU\n",
    " * Leaky ReLU\n",
    " * Softmax\n",
    "* Nuevos paradigmas de redes neuronales:\n",
    " * Reinforcement Learning\n",
    " * Generative Adversarian Networks\n",
    "\n",
    "https://www.asimovinstitute.org/neural-network-zoo/\n",
    "![](https://www.asimovinstitute.org/wp-content/uploads/2019/04/NeuralNetworkZo19High.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más sobre deeplearning:\n",
    "https://beamandrew.github.io/deeplearning/2017/02/23/deep_learning_101_part1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usos de Redes Neuronales\n",
    "\n",
    "Se pueden usar para resolver problemas de:\n",
    "* Clasificación: Imágenes, datos\n",
    "* Regresión\n",
    "* Predicción de series temporables\n",
    "* Generación de imágenes: mediante GAN\n",
    "* Aprendizaje semisupervisado: pueden aprender a resolver problemas complejos como por ejemplo jugar a videojuegos o ganar al Go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
